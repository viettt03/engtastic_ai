{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eff6ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "DATA_DIR = Path(\"datasets\")\n",
    "MODULE = \"BBB\"\n",
    "PRESENTATIONS = [\"2013B\", \"2013J\"]\n",
    "\n",
    "# Bạn có thể chỉnh danh sách cutoff để sinh nhiều sample hơn\n",
    "CUTOFFS = [3, 5, 7, 10, 14, 21, 30, 45, 60, 90, 120, 150, 180]\n",
    "\n",
    "WINDOW_DAYS = 14       # cửa sổ feature nhìn lại 14 ngày\n",
    "HALF_WINDOW = 7        # chia 14 ngày thành 2 nửa: 0-7 và 8-14\n",
    "HORIZON = 7            # dự đoán 7 ngày tương lai\n",
    "VAR_THRESH = 0.0\n",
    "\n",
    "MODEL_PATH = \"short_term_inactive_next7days.pkl\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(asctime)s] %(levelname)s - %(message)s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6180791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: Load raw OULAD ===\n",
    "def load_raw(data_dir: Path) -> Dict[str, pd.DataFrame]:\n",
    "    return {\n",
    "        \"student_info\": pd.read_csv(data_dir / \"studentInfo.csv\"),\n",
    "        \"student_reg\": pd.read_csv(data_dir / \"studentRegistration.csv\"),\n",
    "        \"student_vle\": pd.read_csv(data_dir / \"studentVle.csv\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def prepare_students(raw: Dict[str, pd.DataFrame], module: str, presentations: List[str]):\n",
    "    reg_mod = raw[\"student_reg\"][\n",
    "        (raw[\"student_reg\"][\"code_module\"] == module)\n",
    "        & (raw[\"student_reg\"][\"code_presentation\"].isin(presentations))\n",
    "    ].copy()\n",
    "\n",
    "    reg_lookup = reg_mod[[\"id_student\", \"date_registration\"]].drop_duplicates()\n",
    "\n",
    "    students = raw[\"student_info\"][\n",
    "        (raw[\"student_info\"][\"code_module\"] == module)\n",
    "        & (raw[\"student_info\"][\"code_presentation\"].isin(presentations))\n",
    "        & (raw[\"student_info\"][\"id_student\"].isin(reg_lookup[\"id_student\"]))\n",
    "    ].copy()\n",
    "\n",
    "    vle_mod = raw[\"student_vle\"][\n",
    "        (raw[\"student_vle\"][\"code_module\"] == module)\n",
    "        & (raw[\"student_vle\"][\"code_presentation\"].isin(presentations))\n",
    "    ].merge(reg_lookup, on=\"id_student\", how=\"inner\")\n",
    "\n",
    "    # ngày tương đối kể từ lúc đăng ký (học viên đăng ký muộn vẫn OK)\n",
    "    vle_mod[\"days_since_reg\"] = vle_mod[\"date\"] - vle_mod[\"date_registration\"]\n",
    "\n",
    "    # Chỉ giữ những record có days_since_reg hợp lệ (>=0)\n",
    "    vle_mod = vle_mod[vle_mod[\"days_since_reg\"].notna()].copy()\n",
    "    vle_mod = vle_mod[vle_mod[\"days_since_reg\"] >= 0]\n",
    "\n",
    "    logging.info(\"So hoc vien hop le: %d\", students[\"id_student\"].nunique())\n",
    "    logging.info(\"So ban ghi VLE: %d\", len(vle_mod))\n",
    "    return students, vle_mod\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a823a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3: Helpers ===\n",
    "def compute_inactivity_streak(days_list: List[int], start_day: int, end_day: int) -> int:\n",
    "    \"\"\"\n",
    "    Đếm số ngày liên tiếp KHÔNG hoạt động tính từ end_day lùi về start_day.\n",
    "    \"\"\"\n",
    "    if not days_list:\n",
    "        return end_day - start_day + 1\n",
    "    active = set(days_list)\n",
    "    streak, d = 0, end_day\n",
    "    while d >= start_day and d not in active:\n",
    "        streak += 1\n",
    "        d -= 1\n",
    "    return streak\n",
    "\n",
    "\n",
    "def build_short_term_label(vle_mod: pd.DataFrame, cutoff: int, horizon: int = 7) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Label ngắn hạn:\n",
    "    y_short = 1 nếu KHÔNG có activity trong (cutoff+1 .. cutoff+horizon)\n",
    "    \"\"\"\n",
    "    future = vle_mod[(vle_mod[\"days_since_reg\"] > cutoff) & (vle_mod[\"days_since_reg\"] <= cutoff + horizon)]\n",
    "\n",
    "    has_future_activity = (\n",
    "        future.groupby(\"id_student\")[\"days_since_reg\"].nunique().gt(0).astype(int).reset_index()\n",
    "    ).rename(columns={\"days_since_reg\": \"has_activity_next\"})\n",
    "\n",
    "    # y=1 nếu vắng (không hoạt động)\n",
    "    has_future_activity[\"y_short\"] = (has_future_activity[\"has_activity_next\"] == 0).astype(int)\n",
    "    return has_future_activity[[\"id_student\", \"y_short\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff233d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4: Build features (last 14 days) + label next 7 days ===\n",
    "def build_features_short_term(\n",
    "    students: pd.DataFrame,\n",
    "    vle_mod: pd.DataFrame,\n",
    "    cutoffs: List[int],\n",
    "    window_days: int = WINDOW_DAYS,\n",
    "    half_window: int = HALF_WINDOW,\n",
    "    horizon: int = HORIZON,\n",
    ") -> Tuple[pd.DataFrame, List[str]]:\n",
    "\n",
    "    max_day_by_student = vle_mod.groupby(\"id_student\")[\"days_since_reg\"].max().reset_index()\n",
    "    max_day_lookup = dict(zip(max_day_by_student[\"id_student\"], max_day_by_student[\"days_since_reg\"]))\n",
    "\n",
    "    augmented = []\n",
    "    for cutoff in cutoffs:\n",
    "        # filter cutoff hợp lệ: phải còn đủ \"tương lai 7 ngày\"\n",
    "        # nếu cutoff > (max_day - horizon) thì label tương lai sẽ thiếu => bỏ\n",
    "        valid_students = [sid for sid in students[\"id_student\"].unique()\n",
    "                          if max_day_lookup.get(sid, -1) >= cutoff + horizon]\n",
    "        if not valid_students:\n",
    "            continue\n",
    "\n",
    "        base = pd.DataFrame({\"id_student\": valid_students})\n",
    "        base[\"cutoff\"] = cutoff\n",
    "\n",
    "        # Cửa sổ feature: [cutoff-13 .. cutoff]\n",
    "        w_start = max(0, cutoff - (window_days - 1))\n",
    "        w_end = cutoff\n",
    "\n",
    "        vle_cum = vle_mod[(vle_mod[\"id_student\"].isin(valid_students)) & (vle_mod[\"days_since_reg\"] <= cutoff)].copy()\n",
    "        vle_win = vle_cum[vle_cum[\"days_since_reg\"] >= w_start].copy()\n",
    "\n",
    "        # --- Cum features ---\n",
    "        cum_agg = (\n",
    "            vle_cum.groupby(\"id_student\")\n",
    "            .agg(\n",
    "                total_clicks=(\"sum_click\", \"sum\"),\n",
    "                active_days_total=(\"days_since_reg\", \"nunique\"),\n",
    "                last_active=(\"days_since_reg\", \"max\"),\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        cum_agg[\"days_elapsed_since_reg\"] = cutoff\n",
    "        cum_agg[\"clicks_per_day_total\"] = cum_agg[\"total_clicks\"] / max(cutoff, 1)\n",
    "        cum_agg[\"active_ratio_total\"] = cum_agg[\"active_days_total\"] / max(cutoff, 1)\n",
    "        cum_agg[\"days_since_last_active\"] = cutoff - cum_agg[\"last_active\"]\n",
    "        cum_agg[\"avg_clicks_per_active_day_total\"] = (\n",
    "            cum_agg[\"total_clicks\"] / cum_agg[\"active_days_total\"].replace(0, np.nan)\n",
    "        ).fillna(0)\n",
    "\n",
    "        # --- Window 14 days features ---\n",
    "        win_agg = (\n",
    "            vle_win.groupby(\"id_student\")\n",
    "            .agg(\n",
    "                clicks_last_14_days=(\"sum_click\", \"sum\"),\n",
    "                active_days_14=(\"days_since_reg\", \"nunique\")\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        win_agg[\"clicks_per_day_14\"] = win_agg[\"clicks_last_14_days\"] / window_days\n",
    "        win_agg[\"active_ratio_14\"] = win_agg[\"active_days_14\"] / window_days\n",
    "\n",
    "        # Split 14 days into 0-7 & 8-14 (relative inside window)\n",
    "        first_end = min(w_end, w_start + (half_window - 1))\n",
    "        second_start = min(w_end, first_end + 1)\n",
    "\n",
    "        clicks_0_7 = (\n",
    "            vle_win[(vle_win[\"days_since_reg\"] >= w_start) & (vle_win[\"days_since_reg\"] <= first_end)]\n",
    "            .groupby(\"id_student\")[\"sum_click\"].sum().reset_index(name=\"clicks_0_7\")\n",
    "        )\n",
    "        clicks_8_14 = (\n",
    "            vle_win[(vle_win[\"days_since_reg\"] >= second_start) & (vle_win[\"days_since_reg\"] <= w_end)]\n",
    "            .groupby(\"id_student\")[\"sum_click\"].sum().reset_index(name=\"clicks_8_14\")\n",
    "        )\n",
    "\n",
    "        clicks_last_7 = (\n",
    "            vle_cum[vle_cum[\"days_since_reg\"] > (cutoff - 7)]\n",
    "            .groupby(\"id_student\")[\"sum_click\"].sum().reset_index(name=\"clicks_last_7_days\")\n",
    "        )\n",
    "\n",
    "        # inactivity streak inside last 14 days\n",
    "        days_list = (\n",
    "            vle_win.groupby(\"id_student\")[\"days_since_reg\"]\n",
    "            .apply(lambda x: sorted(x.unique()))\n",
    "            .reset_index()\n",
    "            .rename(columns={\"days_since_reg\": \"active_days_list\"})\n",
    "        )\n",
    "        days_list[\"inactivity_streak_14\"] = days_list[\"active_days_list\"].apply(\n",
    "            lambda lst: compute_inactivity_streak(lst, w_start, w_end)\n",
    "        )\n",
    "        streak = days_list[[\"id_student\", \"inactivity_streak_14\"]]\n",
    "\n",
    "        # label next 7 days\n",
    "        label_df = build_short_term_label(vle_mod[vle_mod[\"id_student\"].isin(valid_students)], cutoff, horizon=horizon)\n",
    "\n",
    "        merged = base.merge(label_df, on=\"id_student\", how=\"left\")\n",
    "        # nếu không có record future (hiếm do mình đã filter), fallback coi như vắng\n",
    "        merged[\"y_short\"] = merged[\"y_short\"].fillna(1).astype(int)\n",
    "\n",
    "        merged = merged.merge(cum_agg, on=\"id_student\", how=\"left\")\n",
    "        merged = merged.merge(win_agg, on=\"id_student\", how=\"left\")\n",
    "        merged = merged.merge(clicks_0_7, on=\"id_student\", how=\"left\")\n",
    "        merged = merged.merge(clicks_8_14, on=\"id_student\", how=\"left\")\n",
    "        merged = merged.merge(clicks_last_7, on=\"id_student\", how=\"left\")\n",
    "        merged = merged.merge(streak, on=\"id_student\", how=\"left\")\n",
    "\n",
    "        # fill missing numeric with 0\n",
    "        fill0 = [\n",
    "            \"total_clicks\",\"active_days_total\",\"last_active\",\n",
    "            \"clicks_last_14_days\",\"active_days_14\",\n",
    "            \"clicks_0_7\",\"clicks_8_14\",\"clicks_last_7_days\",\n",
    "            \"inactivity_streak_14\",\n",
    "            \"days_elapsed_since_reg\",\"clicks_per_day_total\",\"active_ratio_total\",\n",
    "            \"avg_clicks_per_active_day_total\",\"days_since_last_active\",\n",
    "            \"clicks_per_day_14\",\"active_ratio_14\"\n",
    "        ]\n",
    "        for col in fill0:\n",
    "            if col in merged.columns:\n",
    "                merged[col] = merged[col].fillna(0)\n",
    "\n",
    "        merged[\"trend_click_14\"] = merged[\"clicks_8_14\"] - merged[\"clicks_0_7\"]\n",
    "        merged[\"ratio_click_14\"] = (merged[\"clicks_8_14\"] + 1) / (merged[\"clicks_0_7\"] + 1)\n",
    "\n",
    "        augmented.append(merged)\n",
    "\n",
    "    final_df = pd.concat(augmented, ignore_index=True)\n",
    "\n",
    "    feature_cols = [\n",
    "        \"days_elapsed_since_reg\",\n",
    "        \"clicks_per_day_total\",\n",
    "        \"active_ratio_total\",\n",
    "        \"avg_clicks_per_active_day_total\",\n",
    "        \"days_since_last_active\",\n",
    "\n",
    "        \"clicks_last_14_days\",\n",
    "        \"active_days_14\",\n",
    "        \"clicks_per_day_14\",\n",
    "        \"active_ratio_14\",\n",
    "\n",
    "        \"clicks_last_7_days\",\n",
    "        \"clicks_0_7\",\n",
    "        \"clicks_8_14\",\n",
    "        \"trend_click_14\",\n",
    "        \"ratio_click_14\",\n",
    "        \"inactivity_streak_14\",\n",
    "    ]\n",
    "    return final_df, feature_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f879d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: Pipelines + Models ===\n",
    "def make_eval_pipe(model):\n",
    "    return ImbPipeline([\n",
    "        (\"variance_threshold\", VarianceThreshold(VAR_THRESH)),\n",
    "        (\"smote\", SMOTE()),\n",
    "        (\"power_transformer\", PowerTransformer()),\n",
    "        (\"classifier\", model),\n",
    "    ])\n",
    "\n",
    "def make_prod_pipe(model):\n",
    "    return SkPipeline([\n",
    "        (\"variance_threshold\", VarianceThreshold(VAR_THRESH)),\n",
    "        (\"power_transformer\", PowerTransformer()),\n",
    "        (\"classifier\", model),\n",
    "    ])\n",
    "\n",
    "MODELS = {\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=2000,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=12,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\"\n",
    "    ),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=3,\n",
    "        min_samples_leaf=40,\n",
    "        min_samples_split=20,\n",
    "        n_estimators=150,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"MLP\": MLPClassifier(\n",
    "        hidden_layer_sizes=(128,),\n",
    "        max_iter=1200,\n",
    "        early_stopping=True,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bfb1421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-28 01:21:00,292] INFO - So hoc vien hop le: 3960\n",
      "[2025-12-28 01:21:00,292] INFO - So ban ghi VLE: 864034\n",
      "[2025-12-28 01:21:01,640] INFO - Train samples: 40769\n",
      "[2025-12-28 01:21:01,640] INFO - Positive rate (vang): 65.30%\n",
      "/home/trong-viet/Desktop/lms/engtastic_ai/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/trong-viet/Desktop/lms/engtastic_ai/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/trong-viet/Desktop/lms/engtastic_ai/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/trong-viet/Desktop/lms/engtastic_ai/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/trong-viet/Desktop/lms/engtastic_ai/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Hiệu năng trung bình (sort theo F1) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>mean_f1</th>\n",
       "      <th>mean_recall_pos(vang)</th>\n",
       "      <th>mean_specificity</th>\n",
       "      <th>mean_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.858496</td>\n",
       "      <td>0.890249</td>\n",
       "      <td>0.878811</td>\n",
       "      <td>0.820196</td>\n",
       "      <td>0.880828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.856877</td>\n",
       "      <td>0.888557</td>\n",
       "      <td>0.873776</td>\n",
       "      <td>0.825024</td>\n",
       "      <td>0.881583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.856803</td>\n",
       "      <td>0.888478</td>\n",
       "      <td>0.873488</td>\n",
       "      <td>0.825387</td>\n",
       "      <td>0.881371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.854424</td>\n",
       "      <td>0.886139</td>\n",
       "      <td>0.867481</td>\n",
       "      <td>0.829850</td>\n",
       "      <td>0.880574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  mean_accuracy   mean_f1  mean_recall_pos(vang)  \\\n",
       "1        RandomForest       0.858496  0.890249               0.878811   \n",
       "2    GradientBoosting       0.856877  0.888557               0.873776   \n",
       "3                 MLP       0.856803  0.888478               0.873488   \n",
       "0  LogisticRegression       0.854424  0.886139               0.867481   \n",
       "\n",
       "   mean_specificity  mean_auc  \n",
       "1          0.820196  0.880828  \n",
       "2          0.825024  0.881583  \n",
       "3          0.825387  0.881371  \n",
       "0          0.829850  0.880574  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model to: short_term_inactive_next7days.pkl (best=RandomForest)\n"
     ]
    }
   ],
   "source": [
    "# === Cell 6: Train/Eval GroupKFold + Save best ===\n",
    "raw = load_raw(DATA_DIR)\n",
    "students, vle_mod = prepare_students(raw, MODULE, PRESENTATIONS)\n",
    "\n",
    "final_df, feature_cols = build_features_short_term(students, vle_mod, CUTOFFS, WINDOW_DAYS, HALF_WINDOW, HORIZON)\n",
    "\n",
    "X = final_df[feature_cols].fillna(0)\n",
    "y = final_df[\"y_short\"].astype(int)          # 1 = vắng 7 ngày tới\n",
    "groups = final_df[\"id_student\"]\n",
    "\n",
    "logging.info(\"Train samples: %d\", len(final_df))\n",
    "logging.info(\"Positive rate (vang): %.2f%%\", 100 * y.mean())\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "summary_rows = []\n",
    "\n",
    "for name, model in MODELS.items():\n",
    "    fold_rows = []\n",
    "    for tr_idx, te_idx in gkf.split(X, y, groups):\n",
    "        X_tr, X_te = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "        y_tr, y_te = y.iloc[tr_idx], y.iloc[te_idx]\n",
    "\n",
    "        pipe = make_eval_pipe(model)\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "\n",
    "        y_pred = pipe.predict(X_te)\n",
    "        # một số model vẫn có predict_proba, nếu không thì fallback\n",
    "        if hasattr(pipe, \"predict_proba\"):\n",
    "            y_proba = pipe.predict_proba(X_te)[:, 1]\n",
    "            auc = roc_auc_score(y_te, y_proba)\n",
    "        else:\n",
    "            y_proba = None\n",
    "            auc = np.nan\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_te, y_pred).ravel()\n",
    "        fold_rows.append({\n",
    "            \"model\": name,\n",
    "            \"accuracy\": accuracy_score(y_te, y_pred),\n",
    "            \"f1\": f1_score(y_te, y_pred),\n",
    "            \"recall_pos(vang)\": recall_score(y_te, y_pred),               # bắt được ca vắng\n",
    "            \"specificity\": tn / (tn + fp + 1e-9),                        # đúng ca không vắng\n",
    "            \"auc\": auc,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(fold_rows)\n",
    "    summary_rows.append({\n",
    "        \"model\": name,\n",
    "        \"mean_accuracy\": df[\"accuracy\"].mean(),\n",
    "        \"mean_f1\": df[\"f1\"].mean(),\n",
    "        \"mean_recall_pos(vang)\": df[\"recall_pos(vang)\"].mean(),\n",
    "        \"mean_specificity\": df[\"specificity\"].mean(),\n",
    "        \"mean_auc\": df[\"auc\"].mean(),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(\"mean_f1\", ascending=False)\n",
    "print(\"=== Hiệu năng trung bình (sort theo F1) ===\")\n",
    "display(summary_df)\n",
    "\n",
    "best_model_name = summary_df.iloc[0][\"model\"]\n",
    "best_model = MODELS[best_model_name]\n",
    "\n",
    "prod_pipe = make_prod_pipe(best_model)\n",
    "prod_pipe.fit(X, y)\n",
    "\n",
    "joblib.dump({\"pipeline\": prod_pipe, \"feature_cols\": feature_cols}, MODEL_PATH)\n",
    "print(f\"✅ Saved model to: {MODEL_PATH} (best={best_model_name})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb6522ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dự đoán NGẮN HẠN (vắng >7 ngày tới) cho student_id=60 ---\n",
      "P(vắng 7 ngày tới): 24.73%\n",
      "Kết luận: ✅ CÓ KHẢ NĂNG VẪN HỌC\n"
     ]
    }
   ],
   "source": [
    "# === Cell 7: Inference cho 1 học viên (nhập feature JSON) ===\n",
    "bundle = joblib.load(MODEL_PATH)\n",
    "loaded_pipe = bundle[\"pipeline\"]\n",
    "feature_cols = bundle[\"feature_cols\"]\n",
    "\n",
    "student_data = {\n",
    "    \"student_id\": \"60\",\n",
    "    \"days_elapsed_since_reg\": 31,\n",
    "    \"clicks_per_day_total\": 0.16129032258064516,\n",
    "    \"active_ratio_total\": 0.16129032258064516,\n",
    "    \"avg_clicks_per_active_day_total\": 1,\n",
    "    \"days_since_last_active\": 2,\n",
    "    \"clicks_last_14_days\": 3,\n",
    "    \"active_days_14\": 3,\n",
    "    \"clicks_per_day_14\": 0.21428571428571427,\n",
    "    \"active_ratio_14\": 0.21428571428571427,\n",
    "    \"clicks_last_7_days\": 1,\n",
    "    \"clicks_0_7\": 1,\n",
    "    \"clicks_8_14\": 2,\n",
    "    \"trend_click_14\": 1,\n",
    "    \"ratio_click_14\": 1.5,\n",
    "    \"inactivity_streak_14\": 6\n",
    "  }\n",
    "\n",
    "\n",
    "input_df = pd.DataFrame([student_data])[feature_cols].fillna(0)\n",
    "\n",
    "proba_vang = loaded_pipe.predict_proba(input_df)[0, 1]\n",
    "pred = loaded_pipe.predict(input_df)[0]  # 1 = vắng 7 ngày tới\n",
    "\n",
    "print(f\"--- Dự đoán NGẮN HẠN (vắng >7 ngày tới) cho student_id={student_data['student_id']} ---\")\n",
    "print(f\"P(vắng 7 ngày tới): {proba_vang:.2%}\")\n",
    "print(f\"Kết luận: {'⚠️ NGUY CƠ VẮNG' if pred==1 else '✅ CÓ KHẢ NĂNG VẪN HỌC'}\")\n",
    "\n",
    "# Bạn tự chọn ngưỡng cảnh báo\n",
    "THRESHOLD = 0.6\n",
    "if proba_vang >= THRESHOLD:\n",
    "    print(f\"Khuyến nghị: Gửi cảnh báo (threshold={THRESHOLD})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed368e42",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_input_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# fallback\u001b[39;00m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.Series(np.zeros(\u001b[38;5;28mlen\u001b[39m(feature_cols)), index=feature_cols)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m X_full = \u001b[43mclean_input_features\u001b[49m(X[feature_cols].copy())\n\u001b[32m     31\u001b[39m y_full = y.astype(\u001b[38;5;28mint\u001b[39m).copy()\n\u001b[32m     33\u001b[39m all_importances = {}\n",
      "\u001b[31mNameError\u001b[39m: name 'clean_input_features' is not defined"
     ]
    }
   ],
   "source": [
    "# === CELL 2: Feature importance for 4 models (fit full data) + plots ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_importance(model_name: str, fitted_model, feature_cols):\n",
    "    \"\"\"\n",
    "    Returns a pandas Series of importances indexed by feature name.\n",
    "    \"\"\"\n",
    "    # Get final estimator from either sklearn Pipeline or imblearn Pipeline\n",
    "    if hasattr(fitted_model, \"named_steps\"):\n",
    "        clf = fitted_model.named_steps.get(\"clf\", fitted_model)\n",
    "    else:\n",
    "        clf = fitted_model\n",
    "\n",
    "    if model_name == \"LogReg\":\n",
    "        # Logistic importance = abs(coef)\n",
    "        coef = clf.coef_\n",
    "        # binary classification: shape (1, n_features)\n",
    "        imp = np.abs(coef).ravel()\n",
    "        return pd.Series(imp, index=feature_cols)\n",
    "\n",
    "    # Tree-based models\n",
    "    if hasattr(clf, \"feature_importances_\"):\n",
    "        return pd.Series(clf.feature_importances_, index=feature_cols)\n",
    "\n",
    "    # fallback\n",
    "    return pd.Series(np.zeros(len(feature_cols)), index=feature_cols)\n",
    "\n",
    "X_full = clean_input_features(X[feature_cols].copy())\n",
    "y_full = y.astype(int).copy()\n",
    "\n",
    "all_importances = {}\n",
    "\n",
    "for name, model in MODELS.items():\n",
    "    model.fit(X_full, y_full)\n",
    "    imp = get_importance(name, model, feature_cols)\n",
    "\n",
    "    # normalize for easier comparison (sum=1)\n",
    "    s = float(imp.sum())\n",
    "    if s > 0:\n",
    "        imp = imp / s\n",
    "\n",
    "    imp = imp.sort_values(ascending=False)\n",
    "    all_importances[name] = imp\n",
    "\n",
    "    # Print top 10\n",
    "    print(f\"\\n=== {name}: TOP 10 Features ===\")\n",
    "    print(imp.head(10).to_string())\n",
    "\n",
    "    # Plot top 15\n",
    "    topk = imp.head(15)[::-1]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(topk.index, topk.values)\n",
    "    plt.title(f\"Top 15 Feature Importance - {name}\")\n",
    "    plt.xlabel(\"Normalized importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Optional: one combined table (top 10 union)\n",
    "top_union = set()\n",
    "for name, imp in all_importances.items():\n",
    "    top_union.update(list(imp.head(10).index))\n",
    "\n",
    "combined = pd.DataFrame({name: all_importances[name] for name in all_importances}).fillna(0)\n",
    "combined = combined.loc[list(top_union)].sort_index()\n",
    "print(\"\\n=== Combined importance table (union of each model's top-10) ===\")\n",
    "combined\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
