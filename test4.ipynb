{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e197ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# === Cell 1: Imports + Config\n",
    "# =========================================\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, recall_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(asctime)s] %(levelname)s - %(message)s\")\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "DATA_DIR = Path(\"datasets\")\n",
    "MODULE = \"BBB\"\n",
    "PRESENTATIONS = [\"2013B\", \"2013J\"]\n",
    "\n",
    "CUTOFFS = [3, 5, 7, 10, 14, 21, 30, 45, 60, 90, 120, 150, 180]\n",
    "\n",
    "WINDOW_DAYS = 14\n",
    "HALF_WINDOW = 7\n",
    "HORIZON = 14\n",
    "\n",
    "VAR_THRESH = 0.0\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "BUNDLE_PATH = \"short_term_inactive_next14days.bundle.pkl\"\n",
    "\n",
    "# “decline blend” config (đổi nếu bạn muốn)\n",
    "ALPHA_MODEL = 0.70  # 0.7 = ưu tiên model dự đoán bỏ học; 0.3 = decline hỗ trợ\n",
    "DECLINE_WEIGHTS = {\n",
    "    \"drop_ratio\": 0.30,\n",
    "    \"drop_trend\": 0.25,\n",
    "    \"low_last7\": 0.20,\n",
    "    \"high_streak\": 0.15,\n",
    "    \"high_since\": 0.10,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcf5bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# === Cell 2: Load raw OULAD\n",
    "# =========================================\n",
    "def load_raw(data_dir: Path) -> Dict[str, pd.DataFrame]:\n",
    "    return {\n",
    "        \"student_info\": pd.read_csv(data_dir / \"studentInfo.csv\"),\n",
    "        \"student_reg\": pd.read_csv(data_dir / \"studentRegistration.csv\"),\n",
    "        \"student_vle\": pd.read_csv(data_dir / \"studentVle.csv\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def prepare_students(raw: Dict[str, pd.DataFrame], module: str, presentations: List[str]):\n",
    "    reg_mod = raw[\"student_reg\"][\n",
    "        (raw[\"student_reg\"][\"code_module\"] == module)\n",
    "        & (raw[\"student_reg\"][\"code_presentation\"].isin(presentations))\n",
    "    ].copy()\n",
    "\n",
    "    reg_lookup = reg_mod[[\"id_student\", \"date_registration\"]].drop_duplicates()\n",
    "\n",
    "    students = raw[\"student_info\"][\n",
    "        (raw[\"student_info\"][\"code_module\"] == module)\n",
    "        & (raw[\"student_info\"][\"code_presentation\"].isin(presentations))\n",
    "        & (raw[\"student_info\"][\"id_student\"].isin(reg_lookup[\"id_student\"]))\n",
    "    ].copy()\n",
    "\n",
    "    vle_mod = raw[\"student_vle\"][\n",
    "        (raw[\"student_vle\"][\"code_module\"] == module)\n",
    "        & (raw[\"student_vle\"][\"code_presentation\"].isin(presentations))\n",
    "    ].merge(reg_lookup, on=\"id_student\", how=\"inner\")\n",
    "\n",
    "    # relative day since registration\n",
    "    vle_mod[\"days_since_reg\"] = vle_mod[\"date\"] - vle_mod[\"date_registration\"]\n",
    "\n",
    "    vle_mod = vle_mod[vle_mod[\"days_since_reg\"].notna()].copy()\n",
    "    vle_mod = vle_mod[vle_mod[\"days_since_reg\"] >= 0].copy()\n",
    "\n",
    "    logging.info(\"So hoc vien hop le: %d\", students[\"id_student\"].nunique())\n",
    "    logging.info(\"So ban ghi VLE: %d\", len(vle_mod))\n",
    "    return students, vle_mod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68621ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# === Cell 3: Helpers (label + features)\n",
    "# =========================================\n",
    "def compute_inactivity_streak(days_list: List[int], start_day: int, end_day: int) -> int:\n",
    "    \"\"\"\n",
    "    Chuỗi ngày vắng liên tiếp tính từ end_day lùi về start_day.\n",
    "    Nếu không có activity trong window => streak = window_length\n",
    "    \"\"\"\n",
    "    if not days_list:\n",
    "        return end_day - start_day + 1\n",
    "\n",
    "    active = set(days_list)\n",
    "    streak, d = 0, end_day\n",
    "    while d >= start_day and d not in active:\n",
    "        streak += 1\n",
    "        d -= 1\n",
    "    return streak\n",
    "\n",
    "\n",
    "MIN_FUTURE_ACTIVE_DAYS = 1\n",
    "MIN_FUTURE_CLICKS = 3\n",
    "\n",
    "def build_short_term_label(\n",
    "    vle_mod: pd.DataFrame,\n",
    "    cutoff: int,\n",
    "    horizon: int,\n",
    "    min_future_active_days: int = MIN_FUTURE_ACTIVE_DAYS,\n",
    "    min_future_clicks: int = MIN_FUTURE_CLICKS\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    y_short = 1 nếu trong (cutoff, cutoff+horizon] hoạt động rất ít:\n",
    "      - số ngày active <= min_future_active_days  OR\n",
    "      - tổng clicks <= min_future_clicks\n",
    "    \"\"\"\n",
    "    future = vle_mod[(vle_mod[\"days_since_reg\"] > cutoff) & (vle_mod[\"days_since_reg\"] <= cutoff + horizon)]\n",
    "\n",
    "    if len(future) == 0:\n",
    "        return pd.DataFrame(columns=[\"id_student\", \"y_short\"])\n",
    "\n",
    "    fut_agg = (\n",
    "        future.groupby(\"id_student\")\n",
    "        .agg(\n",
    "            future_clicks=(\"sum_click\", \"sum\"),\n",
    "            future_active_days=(\"days_since_reg\", \"nunique\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    fut_agg[\"y_short\"] = (\n",
    "        (fut_agg[\"future_active_days\"] <= min_future_active_days)\n",
    "        | (fut_agg[\"future_clicks\"] <= min_future_clicks)\n",
    "    ).astype(int)\n",
    "\n",
    "    return fut_agg[[\"id_student\", \"y_short\"]]\n",
    "\n",
    "\n",
    "def clean_input_features(df: pd.DataFrame, feature_cols: List[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in feature_cols:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "    out = out.fillna(0)\n",
    "\n",
    "    # clip ratio trong [0,1]\n",
    "    for c in [\"active_ratio_total\", \"active_ratio_14\"]:\n",
    "        if c in out.columns:\n",
    "            out[c] = out[c].clip(0, 1)\n",
    "\n",
    "    # non-negative\n",
    "    for c in out.columns:\n",
    "        out[c] = np.maximum(out[c].values, 0)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_features_short_term(\n",
    "    students: pd.DataFrame,\n",
    "    vle_mod: pd.DataFrame,\n",
    "    cutoffs: List[int],\n",
    "    window_days: int,\n",
    "    half_window: int,\n",
    "    horizon: int,\n",
    ") -> Tuple[pd.DataFrame, List[str]]:\n",
    "\n",
    "    student_ids = students[\"id_student\"].unique()\n",
    "    augmented = []\n",
    "\n",
    "    for cutoff in cutoffs:\n",
    "        w_start = max(0, cutoff - (window_days - 1))\n",
    "        w_end = cutoff\n",
    "\n",
    "        vle_cum = vle_mod[vle_mod[\"days_since_reg\"] <= cutoff].copy()\n",
    "        vle_win = vle_cum[vle_cum[\"days_since_reg\"] >= w_start].copy()\n",
    "\n",
    "        base = pd.DataFrame({\"id_student\": student_ids})\n",
    "        base[\"days_elapsed_since_reg\"] = cutoff\n",
    "\n",
    "        # ---- label ----\n",
    "        label_df = build_short_term_label(vle_mod, cutoff, horizon=horizon)\n",
    "        merged = base.merge(label_df, on=\"id_student\", how=\"left\")\n",
    "\n",
    "        # không có future record => coi là vắng => y_short=1\n",
    "        merged[\"y_short\"] = merged[\"y_short\"].fillna(1).astype(int)\n",
    "\n",
    "        # ---- cumulative agg ----\n",
    "        cum_agg = (\n",
    "            vle_cum.groupby(\"id_student\")\n",
    "            .agg(\n",
    "                total_clicks=(\"sum_click\", \"sum\"),\n",
    "                active_days_total=(\"days_since_reg\", \"nunique\"),\n",
    "                last_active=(\"days_since_reg\", \"max\"),\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        den_total = max(cutoff + 1, 1)\n",
    "        cum_agg[\"clicks_per_day_total\"] = cum_agg[\"total_clicks\"] / den_total\n",
    "        cum_agg[\"active_ratio_total\"] = cum_agg[\"active_days_total\"] / den_total\n",
    "        cum_agg[\"days_since_last_active\"] = cutoff - cum_agg[\"last_active\"]\n",
    "        cum_agg[\"avg_clicks_per_active_day_total\"] = (\n",
    "            cum_agg[\"total_clicks\"] / cum_agg[\"active_days_total\"].replace(0, np.nan)\n",
    "        ).fillna(0)\n",
    "\n",
    "        # ---- window 14 agg ----\n",
    "        win_agg = (\n",
    "            vle_win.groupby(\"id_student\")\n",
    "            .agg(\n",
    "                clicks_last_14_days=(\"sum_click\", \"sum\"),\n",
    "                active_days_14=(\"days_since_reg\", \"nunique\"),\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        win_agg[\"clicks_per_day_14\"] = win_agg[\"clicks_last_14_days\"] / window_days\n",
    "        win_agg[\"active_ratio_14\"] = win_agg[\"active_days_14\"] / window_days\n",
    "\n",
    "        # split 14: [w_start..first_end] và [second_start..w_end]\n",
    "        first_end = min(w_end, w_start + (half_window - 1))\n",
    "        second_start = min(w_end, first_end + 1)\n",
    "\n",
    "        clicks_0_7 = (\n",
    "            vle_win[(vle_win[\"days_since_reg\"] >= w_start) & (vle_win[\"days_since_reg\"] <= first_end)]\n",
    "            .groupby(\"id_student\")[\"sum_click\"]\n",
    "            .sum()\n",
    "            .reset_index(name=\"clicks_0_7\")\n",
    "        )\n",
    "        clicks_8_14 = (\n",
    "            vle_win[(vle_win[\"days_since_reg\"] >= second_start) & (vle_win[\"days_since_reg\"] <= w_end)]\n",
    "            .groupby(\"id_student\")[\"sum_click\"]\n",
    "            .sum()\n",
    "            .reset_index(name=\"clicks_8_14\")\n",
    "        )\n",
    "\n",
    "        clicks_last_7 = (\n",
    "            vle_cum[vle_cum[\"days_since_reg\"] > (cutoff - 7)]\n",
    "            .groupby(\"id_student\")[\"sum_click\"]\n",
    "            .sum()\n",
    "            .reset_index(name=\"clicks_last_7_days\")\n",
    "        )\n",
    "\n",
    "        # inactivity streak in last 14 days\n",
    "        days_list = (\n",
    "            vle_win.groupby(\"id_student\")[\"days_since_reg\"]\n",
    "            .apply(lambda x: sorted(x.unique()))\n",
    "            .reset_index()\n",
    "            .rename(columns={\"days_since_reg\": \"active_days_list\"})\n",
    "        )\n",
    "        days_list[\"inactivity_streak_14\"] = days_list[\"active_days_list\"].apply(\n",
    "            lambda lst: compute_inactivity_streak(lst, w_start, w_end)\n",
    "        )\n",
    "        streak = days_list[[\"id_student\", \"inactivity_streak_14\"]]\n",
    "\n",
    "        # ---- merge ----\n",
    "        merged = merged.merge(cum_agg, on=\"id_student\", how=\"left\")\n",
    "        merged = merged.merge(win_agg, on=\"id_student\", how=\"left\")\n",
    "        merged = merged.merge(clicks_0_7, on=\"id_student\", how=\"left\")\n",
    "        merged = merged.merge(clicks_8_14, on=\"id_student\", how=\"left\")\n",
    "        merged = merged.merge(clicks_last_7, on=\"id_student\", how=\"left\")\n",
    "        merged = merged.merge(streak, on=\"id_student\", how=\"left\")\n",
    "\n",
    "        fill0 = [\n",
    "            \"total_clicks\",\"active_days_total\",\"last_active\",\n",
    "            \"clicks_per_day_total\",\"active_ratio_total\",\"days_since_last_active\",\n",
    "            \"avg_clicks_per_active_day_total\",\n",
    "            \"clicks_last_14_days\",\"active_days_14\",\"clicks_per_day_14\",\"active_ratio_14\",\n",
    "            \"clicks_last_7_days\",\"clicks_0_7\",\"clicks_8_14\",\"inactivity_streak_14\",\n",
    "        ]\n",
    "        for col in fill0:\n",
    "            if col in merged.columns:\n",
    "                merged[col] = merged[col].fillna(0)\n",
    "\n",
    "        merged[\"trend_click_14\"] = merged[\"clicks_8_14\"] - merged[\"clicks_0_7\"]\n",
    "        merged[\"ratio_click_14\"] = (merged[\"clicks_8_14\"] + 1) / (merged[\"clicks_0_7\"] + 1)\n",
    "\n",
    "        merged[\"active_ratio_total\"] = merged[\"active_ratio_total\"].clip(0, 1)\n",
    "        merged[\"active_ratio_14\"] = merged[\"active_ratio_14\"].clip(0, 1)\n",
    "\n",
    "        augmented.append(merged)\n",
    "\n",
    "    final_df = pd.concat(augmented, ignore_index=True)\n",
    "\n",
    "    feature_cols = [\n",
    "        \"days_elapsed_since_reg\",\n",
    "        \"clicks_per_day_total\",\n",
    "        \"active_ratio_total\",\n",
    "        \"avg_clicks_per_active_day_total\",\n",
    "        \"days_since_last_active\",\n",
    "        \"clicks_last_14_days\",\n",
    "        \"active_days_14\",\n",
    "        \"clicks_per_day_14\",\n",
    "        \"active_ratio_14\",\n",
    "        \"clicks_last_7_days\",\n",
    "        \"clicks_0_7\",\n",
    "        \"clicks_8_14\",\n",
    "        \"trend_click_14\",\n",
    "        \"ratio_click_14\",\n",
    "        \"inactivity_streak_14\",\n",
    "    ]\n",
    "    return final_df, feature_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31de6c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# === Cell 4: Decline score + blend + thresholds helpers\n",
    "# =========================================\n",
    "def compute_decline_score(df: pd.DataFrame, weights: Dict[str, float]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Rule-based decline score (0..1) để bắt case “tụt mạnh” (Medium).\n",
    "    \"\"\"\n",
    "    ratio = df[\"ratio_click_14\"].astype(float).clip(0, 3)\n",
    "    trend = df[\"trend_click_14\"].astype(float).clip(-50, 50)\n",
    "    last7  = df[\"clicks_last_7_days\"].astype(float).clip(0, 200)\n",
    "    streak = df[\"inactivity_streak_14\"].astype(float).clip(0, 14)\n",
    "    since  = df[\"days_since_last_active\"].astype(float).clip(0, 60)\n",
    "\n",
    "    drop_ratio  = (1 - (ratio / 1.0)).clip(0, 1)      # ratio<1 => tăng rủi ro\n",
    "    drop_trend  = (-trend / 30).clip(0, 1)            # trend âm => tăng rủi ro\n",
    "    low_last7   = (1 - (last7 / 20)).clip(0, 1)       # 7 ngày gần đây ít => rủi ro\n",
    "    high_streak = (streak / 14).clip(0, 1)\n",
    "    high_since  = (since / 14).clip(0, 1)\n",
    "\n",
    "    score = (\n",
    "        weights[\"drop_ratio\"] * drop_ratio +\n",
    "        weights[\"drop_trend\"] * drop_trend +\n",
    "        weights[\"low_last7\"] * low_last7 +\n",
    "        weights[\"high_streak\"] * high_streak +\n",
    "        weights[\"high_since\"] * high_since\n",
    "    )\n",
    "    return np.clip(score, 0, 1)\n",
    "\n",
    "\n",
    "def find_threshold_for_high_recall(y_true: np.ndarray, proba: np.ndarray, min_precision: float = 0.30):\n",
    "    \"\"\"\n",
    "    Chọn threshold sao cho recall cao nhất nhưng precision >= min_precision.\n",
    "    \"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, proba)\n",
    "    best_thr, best_recall = 0.5, -1\n",
    "\n",
    "    for p, r, t in zip(precision[:-1], recall[:-1], thresholds):\n",
    "        if p >= min_precision and r > best_recall:\n",
    "            best_recall = r\n",
    "            best_thr = float(t)\n",
    "\n",
    "    return best_thr, best_recall\n",
    "\n",
    "\n",
    "def fit_platt_scaler(oof_proba: np.ndarray, y_true: np.ndarray, random_seed: int = 42):\n",
    "    \"\"\"\n",
    "    Platt scaling: fit LogisticRegression trên (oof_proba -> y)\n",
    "    \"\"\"\n",
    "    x = oof_proba.reshape(-1, 1)\n",
    "    clf = LogisticRegression(solver=\"lbfgs\", max_iter=2000, random_state=random_seed)\n",
    "    clf.fit(x, y_true.astype(int))\n",
    "    return clf\n",
    "\n",
    "\n",
    "def apply_platt_scaler(platt_model, proba: np.ndarray):\n",
    "    x = proba.reshape(-1, 1)\n",
    "    return platt_model.predict_proba(x)[:, 1]\n",
    "\n",
    "\n",
    "def make_eval_pipe(model):\n",
    "    return ImbPipeline([\n",
    "        (\"variance_threshold\", VarianceThreshold(VAR_THRESH)),\n",
    "        (\"smote\", SMOTE(random_state=RANDOM_SEED)),\n",
    "        (\"power_transformer\", PowerTransformer()),\n",
    "        (\"classifier\", model),\n",
    "    ])\n",
    "\n",
    "\n",
    "def make_prod_pipe(model):\n",
    "    return SkPipeline([\n",
    "        (\"variance_threshold\", VarianceThreshold(VAR_THRESH)),\n",
    "        (\"power_transformer\", PowerTransformer()),\n",
    "        (\"classifier\", model),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95803459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# === Cell 5: Models\n",
    "# =========================================\n",
    "MODELS = {\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=3000,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=RANDOM_SEED\n",
    "    ),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=400,\n",
    "        max_depth=14,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=4,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\"\n",
    "    ),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(\n",
    "        learning_rate=0.05,\n",
    "        max_depth=3,\n",
    "        min_samples_leaf=30,\n",
    "        min_samples_split=20,\n",
    "        n_estimators=200,\n",
    "        random_state=RANDOM_SEED\n",
    "    ),\n",
    "    \"MLP\": MLPClassifier(\n",
    "        hidden_layer_sizes=(128,),\n",
    "        max_iter=1500,\n",
    "        early_stopping=True,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab87fa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-28 22:29:39,717] INFO - So hoc vien hop le: 3960\n",
      "[2025-12-28 22:29:39,718] INFO - So ban ghi VLE: 864034\n",
      "[2025-12-28 22:29:41,068] INFO - Train samples: 51480\n",
      "[2025-12-28 22:29:41,070] INFO - Positive rate (vang_14days): 71.77%\n",
      "/home/trong-viet/Desktop/lms/engtastic_ai/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/trong-viet/Desktop/lms/engtastic_ai/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/trong-viet/Desktop/lms/engtastic_ai/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/trong-viet/Desktop/lms/engtastic_ai/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "/home/trong-viet/Desktop/lms/engtastic_ai/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>mean_f1</th>\n",
       "      <th>mean_recall_pos(vang)</th>\n",
       "      <th>mean_specificity</th>\n",
       "      <th>mean_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.861888</td>\n",
       "      <td>0.904064</td>\n",
       "      <td>0.906962</td>\n",
       "      <td>0.747212</td>\n",
       "      <td>0.874004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.859751</td>\n",
       "      <td>0.902104</td>\n",
       "      <td>0.900739</td>\n",
       "      <td>0.755213</td>\n",
       "      <td>0.871031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.859538</td>\n",
       "      <td>0.902092</td>\n",
       "      <td>0.901864</td>\n",
       "      <td>0.751837</td>\n",
       "      <td>0.852395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.858683</td>\n",
       "      <td>0.901105</td>\n",
       "      <td>0.897286</td>\n",
       "      <td>0.760353</td>\n",
       "      <td>0.874960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  mean_accuracy   mean_f1  mean_recall_pos(vang)  \\\n",
       "1        RandomForest       0.861888  0.904064               0.906962   \n",
       "3                 MLP       0.859751  0.902104               0.900739   \n",
       "0  LogisticRegression       0.859538  0.902092               0.901864   \n",
       "2    GradientBoosting       0.858683  0.901105               0.897286   \n",
       "\n",
       "   mean_specificity  mean_auc  \n",
       "1          0.747212  0.874004  \n",
       "3          0.755213  0.871031  \n",
       "0          0.751837  0.852395  \n",
       "2          0.760353  0.874960  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best model: RandomForest\n",
      "✅ RISK raw thr_med=0.000, thr_high=0.000\n",
      "✅ RISK cal thr_med=0.060, thr_high=0.060\n",
      "✅ Raw thr_med=0.000, thr_high=0.000\n",
      "✅ Cal thr_med=0.086, thr_high=0.086\n",
      "✅ Saved bundle to: short_term_inactive_next14days.bundle.pkl\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# === Cell 6: Train/Eval GroupKFold + Choose best + Save bundle\n",
    "# =========================================\n",
    "raw = load_raw(DATA_DIR)\n",
    "students, vle_mod = prepare_students(raw, MODULE, PRESENTATIONS)\n",
    "\n",
    "final_df, feature_cols = build_features_short_term(\n",
    "    students, vle_mod, CUTOFFS,\n",
    "    window_days=WINDOW_DAYS, half_window=HALF_WINDOW, horizon=HORIZON\n",
    ")\n",
    "\n",
    "X = clean_input_features(final_df[feature_cols].copy(), feature_cols)\n",
    "y = final_df[\"y_short\"].astype(int).values\n",
    "groups = final_df[\"id_student\"].values\n",
    "\n",
    "logging.info(\"Train samples: %d\", len(final_df))\n",
    "logging.info(\"Positive rate (vang_14days): %.2f%%\", 100 * y.mean())\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "summary_rows = []\n",
    "oof_store = {}\n",
    "\n",
    "for name, model in MODELS.items():\n",
    "    fold_rows = []\n",
    "    oof_proba = np.zeros(len(X), dtype=float)\n",
    "\n",
    "    for tr_idx, te_idx in gkf.split(X, y, groups):\n",
    "        X_tr, X_te = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "\n",
    "        pipe = make_eval_pipe(model)\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "\n",
    "        y_pred = pipe.predict(X_te)\n",
    "        proba = pipe.predict_proba(X_te)[:, 1]\n",
    "        oof_proba[te_idx] = proba\n",
    "\n",
    "        auc = roc_auc_score(y_te, proba)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_te, y_pred).ravel()\n",
    "\n",
    "        fold_rows.append({\n",
    "            \"model\": name,\n",
    "            \"accuracy\": accuracy_score(y_te, y_pred),\n",
    "            \"f1\": f1_score(y_te, y_pred),\n",
    "            \"recall_pos(vang)\": recall_score(y_te, y_pred),\n",
    "            \"specificity\": tn / (tn + fp + 1e-9),\n",
    "            \"auc\": auc,\n",
    "        })\n",
    "\n",
    "    df_fold = pd.DataFrame(fold_rows)\n",
    "    summary_rows.append({\n",
    "        \"model\": name,\n",
    "        \"mean_accuracy\": df_fold[\"accuracy\"].mean(),\n",
    "        \"mean_f1\": df_fold[\"f1\"].mean(),\n",
    "        \"mean_recall_pos(vang)\": df_fold[\"recall_pos(vang)\"].mean(),\n",
    "        \"mean_specificity\": df_fold[\"specificity\"].mean(),\n",
    "        \"mean_auc\": df_fold[\"auc\"].mean(),\n",
    "    })\n",
    "    oof_store[name] = oof_proba\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(\"mean_f1\", ascending=False)\n",
    "display(summary_df)\n",
    "\n",
    "best_model_name = summary_df.iloc[0][\"model\"]\n",
    "best_model = MODELS[best_model_name]\n",
    "print(\"✅ Best model:\", best_model_name)\n",
    "\n",
    "# --- OOF -> calibrate ---\n",
    "best_oof = oof_store[best_model_name]\n",
    "platt = fit_platt_scaler(best_oof, y, random_seed=RANDOM_SEED)\n",
    "best_oof_cal = apply_platt_scaler(platt, best_oof)\n",
    "\n",
    "# --- compute decline on full training rows ---\n",
    "decline_all = compute_decline_score(final_df, DECLINE_WEIGHTS)\n",
    "\n",
    "# --- build blended risk OOF (raw & calibrated) ---\n",
    "risk_oof_raw = ALPHA_MODEL * best_oof + (1 - ALPHA_MODEL) * decline_all\n",
    "risk_oof_cal = ALPHA_MODEL * best_oof_cal + (1 - ALPHA_MODEL) * decline_all\n",
    "\n",
    "# --- choose thresholds ON RISK (not proba) ---\n",
    "risk_thr_high_raw, _ = find_threshold_for_high_recall(y, risk_oof_raw, min_precision=0.45)\n",
    "risk_thr_high_cal, _ = find_threshold_for_high_recall(y, risk_oof_cal, min_precision=0.45)\n",
    "\n",
    "risk_thr_med_raw, _  = find_threshold_for_high_recall(y, risk_oof_raw, min_precision=0.30)\n",
    "risk_thr_med_cal, _  = find_threshold_for_high_recall(y, risk_oof_cal, min_precision=0.30)\n",
    "\n",
    "print(f\"✅ RISK raw thr_med={risk_thr_med_raw:.3f}, thr_high={risk_thr_high_raw:.3f}\")\n",
    "print(f\"✅ RISK cal thr_med={risk_thr_med_cal:.3f}, thr_high={risk_thr_high_cal:.3f}\")\n",
    "\n",
    "\n",
    "# Threshold cho “cảnh báo bỏ học” (High) ưu tiên recall nhưng giữ precision tối thiểu\n",
    "thr_high_raw, rec_high_raw = find_threshold_for_high_recall(y, best_oof, min_precision=0.45)\n",
    "thr_high_cal, rec_high_cal = find_threshold_for_high_recall(y, best_oof_cal, min_precision=0.45)\n",
    "\n",
    "# Threshold cho “nhắc nhở sớm” (Medium) dễ hơn (precision thấp hơn)\n",
    "thr_med_raw, _ = find_threshold_for_high_recall(y, best_oof, min_precision=0.30)\n",
    "thr_med_cal, _ = find_threshold_for_high_recall(y, best_oof_cal, min_precision=0.30)\n",
    "\n",
    "print(f\"✅ Raw thr_med={thr_med_raw:.3f}, thr_high={thr_high_raw:.3f}\")\n",
    "print(f\"✅ Cal thr_med={thr_med_cal:.3f}, thr_high={thr_high_cal:.3f}\")\n",
    "\n",
    "# --- train production pipeline on full data ---\n",
    "prod_pipe = make_prod_pipe(best_model)\n",
    "prod_pipe.fit(X, y)\n",
    "\n",
    "bundle = {\n",
    "    \"version\": \"v1\",\n",
    "    \"module\": MODULE,\n",
    "    \"presentations\": PRESENTATIONS,\n",
    "    \"horizon_days\": HORIZON,\n",
    "    \"window_days\": WINDOW_DAYS,\n",
    "\n",
    "    \"pipeline\": prod_pipe,\n",
    "    \"feature_cols\": feature_cols,\n",
    "\n",
    "    # calibrator\n",
    "    \"platt\": platt,\n",
    "\n",
    "    # blend config\n",
    "    \"blend_params\": {\n",
    "        \"alpha_model\": float(ALPHA_MODEL),\n",
    "    },\n",
    "    \"decline_cfg\": {\n",
    "        \"weights\": DECLINE_WEIGHTS,\n",
    "    },\n",
    "\n",
    "    # thresholds (ưu tiên dùng calibrated)\n",
    "    \"thresholds\": {\n",
    "        \"raw\": {\n",
    "            \"medium\": float(thr_med_raw),\n",
    "            \"high\": float(thr_high_raw),\n",
    "        },\n",
    "        \"cal\": {\n",
    "            \"medium\": float(thr_med_cal),\n",
    "            \"high\": float(thr_high_cal),\n",
    "        },\n",
    "        \"notes\": \"Use calibrated thresholds for production.\"\n",
    "    },\n",
    "}\n",
    "\n",
    "joblib.dump(bundle, BUNDLE_PATH)\n",
    "print(f\"✅ Saved bundle to: {BUNDLE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60065359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>proba</th>\n",
       "      <th>decline</th>\n",
       "      <th>risk</th>\n",
       "      <th>risk_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.108018</td>\n",
       "      <td>0.106623</td>\n",
       "      <td>0.107600</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.157859</td>\n",
       "      <td>0.662857</td>\n",
       "      <td>0.309359</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0.354062</td>\n",
       "      <td>0.275714</td>\n",
       "      <td>0.330558</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  student_id     proba   decline      risk risk_level\n",
       "0          2  0.108018  0.106623  0.107600       high\n",
       "1          6  0.157859  0.662857  0.309359       high\n",
       "2          7  0.354062  0.275714  0.330558       high"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================================\n",
    "# === Cell 7: Demo predict on your samples (Low/Medium/High)\n",
    "# =========================================\n",
    "def predict_risk_from_bundle(bundle: dict, df: pd.DataFrame, use_calibrated: bool = True) -> pd.DataFrame:\n",
    "    cols = bundle[\"feature_cols\"]\n",
    "    pipe = bundle[\"pipeline\"]\n",
    "    platt = bundle[\"platt\"]\n",
    "    alpha = float(bundle[\"blend_params\"][\"alpha_model\"])\n",
    "    weights = bundle[\"decline_cfg\"][\"weights\"]\n",
    "\n",
    "    X = clean_input_features(df[cols].copy(), cols)\n",
    "\n",
    "    proba_raw = pipe.predict_proba(X)[:, 1]\n",
    "    proba = apply_platt_scaler(platt, proba_raw) if use_calibrated else proba_raw\n",
    "\n",
    "    decline = compute_decline_score(df, weights)\n",
    "    risk = alpha * proba + (1 - alpha) * decline\n",
    "\n",
    "    bundle[\"risk_thresholds\"] = {\n",
    "      \"raw\": {\"medium\": float(risk_thr_med_raw), \"high\": float(risk_thr_high_raw)},\n",
    "      \"cal\": {\"medium\": float(risk_thr_med_cal), \"high\": float(risk_thr_high_cal)},\n",
    "      \"notes\": \"Thresholds computed on blended risk (alpha*proba + (1-alpha)*decline).\"\n",
    "    }\n",
    "    \n",
    "    thr = bundle[\"risk_thresholds\"][\"cal\"] if use_calibrated else bundle[\"risk_thresholds\"][\"raw\"]\n",
    "    thr_m, thr_h = float(thr[\"medium\"]), float(thr[\"high\"])\n",
    "\n",
    "    level = np.where(risk >= thr_h, \"high\",\n",
    "             np.where(risk >= thr_m, \"medium\", \"low\"))\n",
    "\n",
    "    out = df[[\"student_id\"]].copy()\n",
    "    out[\"proba\"] = proba\n",
    "    out[\"decline\"] = decline\n",
    "    out[\"risk\"] = risk\n",
    "    out[\"risk_level\"] = level\n",
    "    return out\n",
    "\n",
    "\n",
    "samples = [\n",
    "  {\n",
    "    \"student_id\": \"2\",\n",
    "    \"days_elapsed_since_reg\": 32,\n",
    "    \"clicks_per_day_total\": 2.3125,\n",
    "    \"active_ratio_total\": 0.71875,\n",
    "    \"avg_clicks_per_active_day_total\": 3.217391304347826,\n",
    "    \"days_since_last_active\": 0,\n",
    "    \"clicks_last_14_days\": 39,\n",
    "    \"active_days_14\": 11,\n",
    "    \"clicks_per_day_14\": 2.7857142857142856,\n",
    "    \"active_ratio_14\": 0.7857142857142857,\n",
    "    \"clicks_last_7_days\": 17,\n",
    "    \"clicks_0_7\": 21,\n",
    "    \"clicks_8_14\": 18,\n",
    "    \"trend_click_14\": -3,\n",
    "    \"ratio_click_14\": 0.8636363636363636,\n",
    "    \"inactivity_streak_14\": 1\n",
    "  },\n",
    "  {\n",
    "    \"student_id\": \"6\",\n",
    "    \"days_elapsed_since_reg\": 27,\n",
    "    \"clicks_per_day_total\": 1.5555555555555556,\n",
    "    \"active_ratio_total\": 0.4444444444444444,\n",
    "    \"avg_clicks_per_active_day_total\": 3.5,\n",
    "    \"days_since_last_active\": 2,\n",
    "    \"clicks_last_14_days\": 30,\n",
    "    \"active_days_14\": 9,\n",
    "    \"clicks_per_day_14\": 2.142857142857143,\n",
    "    \"active_ratio_14\": 0.6428571428571429,\n",
    "    \"clicks_last_7_days\": 3,\n",
    "    \"clicks_0_7\": 27,\n",
    "    \"clicks_8_14\": 3,\n",
    "    \"trend_click_14\": -24,\n",
    "    \"ratio_click_14\": 0.14285714285714285,\n",
    "    \"inactivity_streak_14\": 2\n",
    "  },\n",
    "  {\n",
    "    \"student_id\": \"7\",\n",
    "    \"days_elapsed_since_reg\": 32,\n",
    "    \"clicks_per_day_total\": 0.15625,\n",
    "    \"active_ratio_total\": 0.15625,\n",
    "    \"avg_clicks_per_active_day_total\": 1,\n",
    "    \"days_since_last_active\": 3,\n",
    "    \"clicks_last_14_days\": 2,\n",
    "    \"active_days_14\": 2,\n",
    "    \"clicks_per_day_14\": 0.14285714285714285,\n",
    "    \"active_ratio_14\": 0.14285714285714285,\n",
    "    \"clicks_last_7_days\": 1,\n",
    "    \"clicks_0_7\": 1,\n",
    "    \"clicks_8_14\": 1,\n",
    "    \"trend_click_14\": 0,\n",
    "    \"ratio_click_14\": 1,\n",
    "    \"inactivity_streak_14\": 6\n",
    "  }\n",
    "]\n",
    "\n",
    "demo_df = pd.DataFrame(samples)\n",
    "bundle = joblib.load(BUNDLE_PATH)\n",
    "\n",
    "pred_demo = predict_risk_from_bundle(bundle, demo_df, use_calibrated=True)\n",
    "display(pred_demo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0033c80c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
