{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eff6ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import f1_score, recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6180791",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"datasets\")\n",
    "MODULE = \"BBB\"\n",
    "PRESENTATIONS = [\"2013B\", \"2013J\"]\n",
    "CUTOFFS = [14, 21, 30, 45, 60, 90, 120, 150]\n",
    "LOOK_AHEAD = 14\n",
    "MODEL_PATH = \"short_term_warning_model.pkl\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(asctime)s] %(levelname)s - %(message)s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a823a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_inactivity_streak(days_list: List[int], start_day: int, end_day: int) -> int:\n",
    "    if not days_list:\n",
    "        return end_day - start_day + 1\n",
    "    active = set(days_list)\n",
    "    streak, d = 0, end_day\n",
    "    while d >= start_day and d not in active:\n",
    "        streak += 1\n",
    "        d -= 1\n",
    "    return streak\n",
    "\n",
    "def load_raw(data_dir: Path):\n",
    "    return {\n",
    "        \"info\": pd.read_csv(data_dir / \"studentInfo.csv\"),\n",
    "        \"reg\": pd.read_csv(data_dir / \"studentRegistration.csv\"),\n",
    "        \"vle\": pd.read_csv(data_dir / \"studentVle.csv\"),\n",
    "    }\n",
    "\n",
    "def build_short_term_dataset(raw: Dict[str, pd.DataFrame], cutoffs: List[int]):\n",
    "    reg = raw[\"reg\"][\n",
    "        (raw[\"reg\"][\"code_module\"] == MODULE) &\n",
    "        (raw[\"reg\"][\"code_presentation\"].isin(PRESENTATIONS))\n",
    "    ].copy()\n",
    "\n",
    "    reg[\"date_unregistration\"] = pd.to_numeric(reg[\"date_unregistration\"], errors=\"coerce\").fillna(999)\n",
    "    reg[\"date_registration\"] = pd.to_numeric(reg[\"date_registration\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    reg_lookup = reg[[\"id_student\", \"date_registration\", \"date_unregistration\"]].drop_duplicates()\n",
    "\n",
    "    vle = raw[\"vle\"].merge(reg_lookup[[\"id_student\", \"date_registration\"]], on=\"id_student\", how=\"inner\")\n",
    "    vle[\"date\"] = pd.to_numeric(vle[\"date\"], errors=\"coerce\")\n",
    "    vle[\"sum_click\"] = pd.to_numeric(vle[\"sum_click\"], errors=\"coerce\").fillna(0)\n",
    "    vle = vle.dropna(subset=[\"date\"])\n",
    "    vle[\"days_since_reg\"] = vle[\"date\"] - vle[\"date_registration\"]\n",
    "\n",
    "    all_samples = []\n",
    "\n",
    "    for cutoff in cutoffs:\n",
    "        current_batch = reg_lookup[reg_lookup[\"date_unregistration\"] > cutoff].copy()\n",
    "\n",
    "        # target: sáº½ unreg trong (cutoff, cutoff+LOOK_AHEAD]\n",
    "        current_batch[\"target\"] = (\n",
    "            (current_batch[\"date_unregistration\"] > cutoff) &\n",
    "            (current_batch[\"date_unregistration\"] <= cutoff + LOOK_AHEAD)\n",
    "        ).astype(int)\n",
    "\n",
    "        vle_cum = vle[\n",
    "            (vle[\"id_student\"].isin(current_batch[\"id_student\"])) &\n",
    "            (vle[\"days_since_reg\"] <= cutoff) &\n",
    "            (vle[\"days_since_reg\"] >= 0)\n",
    "        ].copy()\n",
    "\n",
    "        vle_win = vle_cum[vle_cum[\"days_since_reg\"] >= (cutoff - 13)].copy()\n",
    "\n",
    "        # Total features\n",
    "        agg_total = vle_cum.groupby(\"id_student\").agg(\n",
    "            total_clicks=(\"sum_click\", \"sum\"),\n",
    "            active_days_total=(\"days_since_reg\", \"nunique\"),\n",
    "            last_active=(\"days_since_reg\", \"max\"),\n",
    "        ).reset_index()\n",
    "\n",
    "        denom_total = max(cutoff, 1)\n",
    "        agg_total[\"clicks_per_day_total\"] = agg_total[\"total_clicks\"] / denom_total\n",
    "        agg_total[\"active_ratio_total\"] = agg_total[\"active_days_total\"] / denom_total\n",
    "        agg_total[\"days_since_last_active\"] = cutoff - agg_total[\"last_active\"]\n",
    "        agg_total[\"avg_clicks_per_active_day_total\"] = (\n",
    "            agg_total[\"total_clicks\"] / agg_total[\"active_days_total\"]\n",
    "        ).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "        # 14-day window features\n",
    "        agg_14 = vle_win.groupby(\"id_student\").agg(\n",
    "            clicks_last_14_days=(\"sum_click\", \"sum\"),\n",
    "            active_days_14=(\"days_since_reg\", \"nunique\"),\n",
    "        ).reset_index()\n",
    "\n",
    "        agg_14[\"clicks_per_day_14\"] = agg_14[\"clicks_last_14_days\"] / 14\n",
    "        agg_14[\"active_ratio_14\"] = agg_14[\"active_days_14\"] / 14\n",
    "\n",
    "        # Trend/streak in window\n",
    "        c_0_7 = vle_win[vle_win[\"days_since_reg\"] <= (cutoff - 7)].groupby(\"id_student\")[\"sum_click\"].sum().reset_index(name=\"clicks_0_7\")\n",
    "        c_8_14 = vle_win[vle_win[\"days_since_reg\"] > (cutoff - 7)].groupby(\"id_student\")[\"sum_click\"].sum().reset_index(name=\"clicks_8_14\")\n",
    "        c_last_7 = vle_cum[vle_cum[\"days_since_reg\"] > (cutoff - 7)].groupby(\"id_student\")[\"sum_click\"].sum().reset_index(name=\"clicks_last_7_days\")\n",
    "\n",
    "        streak_df = vle_win.groupby(\"id_student\")[\"days_since_reg\"].apply(lambda x: list(set(x))).reset_index()\n",
    "        streak_df[\"inactivity_streak_14\"] = streak_df[\"days_since_reg\"].apply(lambda x: compute_inactivity_streak(x, cutoff - 13, cutoff))\n",
    "\n",
    "        merged = current_batch[[\"id_student\", \"target\"]].merge(agg_total, on=\"id_student\", how=\"left\")\n",
    "        merged = merged.merge(agg_14, on=\"id_student\", how=\"left\")\n",
    "        merged = merged.merge(c_0_7, on=\"id_student\", how=\"left\")\n",
    "        merged = merged.merge(c_8_14, on=\"id_student\", how=\"left\")\n",
    "        merged = merged.merge(c_last_7, on=\"id_student\", how=\"left\")\n",
    "        merged = merged.merge(streak_df[[\"id_student\", \"inactivity_streak_14\"]], on=\"id_student\", how=\"left\")\n",
    "\n",
    "        merged.iloc[:, 2:] = merged.iloc[:, 2:].fillna(0)\n",
    "\n",
    "        merged[\"trend_click_14\"] = merged[\"clicks_8_14\"] - merged[\"clicks_0_7\"]\n",
    "        merged[\"ratio_click_14\"] = (merged[\"clicks_8_14\"] + 1) / (merged[\"clicks_0_7\"] + 1)\n",
    "        merged[\"days_elapsed_since_reg\"] = cutoff\n",
    "\n",
    "        # CLIP ratios to valid ranges (trÃ¡nh bug kiá»ƒu Nest > 1)\n",
    "        merged[\"active_ratio_total\"] = merged[\"active_ratio_total\"].clip(0, 1)\n",
    "        merged[\"active_ratio_14\"] = merged[\"active_ratio_14\"].clip(0, 1)\n",
    "\n",
    "        all_samples.append(merged)\n",
    "\n",
    "    final_df = pd.concat(all_samples, ignore_index=True)\n",
    "\n",
    "    feature_cols = [\n",
    "        \"days_elapsed_since_reg\",\n",
    "        \"clicks_per_day_total\", \"active_ratio_total\",\n",
    "        \"avg_clicks_per_active_day_total\", \"days_since_last_active\",\n",
    "        \"clicks_last_14_days\", \"active_days_14\",\n",
    "        \"clicks_per_day_14\", \"active_ratio_14\",\n",
    "        \"clicks_last_7_days\", \"clicks_0_7\", \"clicks_8_14\",\n",
    "        \"trend_click_14\", \"ratio_click_14\",\n",
    "        \"inactivity_streak_14\",\n",
    "    ]\n",
    "    return final_df, feature_cols\n",
    "\n",
    "def clean_input_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # Ã©p numeric + fill\n",
    "    for c in out.columns:\n",
    "        out[c] = pd.to_numeric(out[c], errors=\"coerce\")\n",
    "    out = out.fillna(0)\n",
    "\n",
    "    # clip ratios\n",
    "    if \"active_ratio_total\" in out: out[\"active_ratio_total\"] = out[\"active_ratio_total\"].clip(0, 1)\n",
    "    if \"active_ratio_14\" in out: out[\"active_ratio_14\"] = out[\"active_ratio_14\"].clip(0, 1)\n",
    "\n",
    "    # clip streak [0,14]\n",
    "    if \"inactivity_streak_14\" in out: out[\"inactivity_streak_14\"] = out[\"inactivity_streak_14\"].clip(0, 14)\n",
    "\n",
    "    # khÃ´ng cho giÃ¡ trá»‹ Ã¢m á»Ÿ clicks\n",
    "    for c in out.columns:\n",
    "        if \"click\" in c:\n",
    "            out[c] = out[c].clip(lower=0)\n",
    "\n",
    "    # days_since_last_active >=0\n",
    "    if \"days_since_last_active\" in out: out[\"days_since_last_active\"] = out[\"days_since_last_active\"].clip(lower=0)\n",
    "\n",
    "    return out\n",
    "\n",
    "def heuristic_risk(row: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Heuristic nháº¹ Ä‘á»ƒ Ä‘áº£m báº£o: Ã­t hoáº¡t Ä‘á»™ng + streak cao -> risk cao.\n",
    "    Tráº£ vá» [0,1]\n",
    "    \"\"\"\n",
    "    streak = row[\"inactivity_streak_14\"] / 14.0                 # 0..1\n",
    "    low_click14 = 1.0 - np.tanh(row[\"clicks_last_14_days\"] / 10) # cÃ ng Ã­t click -> cÃ ng cao\n",
    "    last7_zero = 1.0 if row[\"clicks_last_7_days\"] <= 1 else 0.0\n",
    "    days_last = np.tanh(row[\"days_since_last_active\"] / 5)       # 0..~1\n",
    "\n",
    "    score = 0.45*streak + 0.30*low_click14 + 0.15*last7_zero + 0.10*days_last\n",
    "    return float(np.clip(score, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff233d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = load_raw(DATA_DIR)\n",
    "final_df, feature_cols = build_short_term_dataset(raw, CUTOFFS)\n",
    "\n",
    "X = final_df[feature_cols]\n",
    "y = final_df[\"target\"].astype(int)\n",
    "groups = final_df[\"id_student\"]\n",
    "\n",
    "X = clean_input_features(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f879d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-27 21:38:25,053] INFO - LogReg: MeanF1=0.0538 | MedianThr=0.5895\n",
      "[2025-12-27 21:38:34,708] INFO - RF: MeanF1=0.0308 | MedianThr=0.5044\n",
      "[2025-12-27 21:38:35,058] INFO - Saved: LogReg, F1=0.0538, threshold=0.5895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "CANDIDATES = {\n",
    "    \"LogReg\": Pipeline([\n",
    "        (\"pt\", PowerTransformer()),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            max_iter=3000,\n",
    "            class_weight=\"balanced\",\n",
    "            C=2.0,\n",
    "            solver=\"lbfgs\"\n",
    "        ))\n",
    "    ]),\n",
    "    \"RF\": ImbPipeline([\n",
    "        (\"smote\", SMOTE(sampling_strategy=0.6, random_state=42)),\n",
    "        (\"clf\", RandomForestClassifier(\n",
    "            n_estimators=500,\n",
    "            max_depth=10,\n",
    "            min_samples_leaf=10,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "def best_threshold_by_f1(y_true, proba):\n",
    "    p, r, th = precision_recall_curve(y_true, proba)\n",
    "    f1 = 2*p*r/(p+r+1e-12)\n",
    "    # precision_recall_curve tráº£ th cÃ³ length = len(p)-1\n",
    "    best_idx = np.nanargmax(f1[:-1])\n",
    "    return float(th[best_idx]), float(f1[best_idx])\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "best_name, best_f1 = None, -1\n",
    "best_model, best_thr = None, 0.5\n",
    "\n",
    "for name, model in CANDIDATES.items():\n",
    "    fold_f1s = []\n",
    "    fold_thrs = []\n",
    "\n",
    "    for tr_idx, te_idx in gkf.split(X, y, groups):\n",
    "        Xtr, Xte = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "        ytr, yte = y.iloc[tr_idx], y.iloc[te_idx]\n",
    "\n",
    "        model.fit(Xtr, ytr)\n",
    "\n",
    "        # chá»n threshold tá»‘i Æ°u trÃªn train-fold\n",
    "        proba_tr = model.predict_proba(Xtr)[:, 1]\n",
    "        thr, _ = best_threshold_by_f1(ytr, proba_tr)\n",
    "\n",
    "        # cháº¥m trÃªn test-fold vá»›i threshold Ä‘Ã³\n",
    "        proba_te = model.predict_proba(Xte)[:, 1]\n",
    "        pred_te = (proba_te >= thr).astype(int)\n",
    "\n",
    "        fold_f1s.append(f1_score(yte, pred_te))\n",
    "        fold_thrs.append(thr)\n",
    "\n",
    "    mean_f1 = float(np.mean(fold_f1s))\n",
    "    mean_thr = float(np.median(fold_thrs))\n",
    "    logging.info(f\"{name}: MeanF1={mean_f1:.4f} | MedianThr={mean_thr:.4f}\")\n",
    "\n",
    "    if mean_f1 > best_f1:\n",
    "        best_f1 = mean_f1\n",
    "        best_name = name\n",
    "        best_model = model\n",
    "        best_thr = mean_thr\n",
    "\n",
    "# fit full + save model + threshold\n",
    "best_model.fit(X, y)\n",
    "joblib.dump(\n",
    "    {\"model\": best_model, \"feature_cols\": feature_cols, \"threshold\": best_thr},\n",
    "    MODEL_PATH\n",
    ")\n",
    "logging.info(f\"Saved: {best_name}, F1={best_f1:.4f}, threshold={best_thr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bfb1421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive rate: 0.01702863732462041\n",
      "target\n",
      "0    25572\n",
      "1      443\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive rate:\", y.mean())\n",
    "print(y.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6522ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  student_id  model_proba  heur_boosted  raw_score  display_risk  final_pred\n",
      "2         60     0.586206      0.551593   0.570631      0.637619           0\n",
      "1         59     0.443094      0.095209   0.286545      0.153471           0\n",
      "0         25     0.363012      0.035142   0.215471      0.093110           0\n"
     ]
    }
   ],
   "source": [
    "def predict_risk(bundle, input_df: pd.DataFrame,\n",
    "                 alpha_model: float = 0.55,\n",
    "                 heur_boost: float = 1.15,\n",
    "                 k: float = 8.0,      # Ä‘á»™ dá»‘c\n",
    "                 c: float = 0.50):    # Ä‘iá»ƒm giá»¯a\n",
    "    model = bundle[\"model\"]\n",
    "    cols = bundle[\"feature_cols\"]\n",
    "\n",
    "    df = input_df.copy()\n",
    "    df = df.rename(columns={\"student_id\": \"id_student\"})\n",
    "    df = df[cols]\n",
    "    df = clean_input_features(df)\n",
    "\n",
    "    proba = model.predict_proba(df)[:, 1]\n",
    "    heur = df.apply(heuristic_risk, axis=1).values\n",
    "\n",
    "    heur2 = np.clip(heur * heur_boost, 0, 1)\n",
    "    raw = alpha_model * proba + (1 - alpha_model) * heur2\n",
    "\n",
    "    # map raw -> display probability (0..1), giÃºp nhÃ¬n \"Ä‘Ã£\"\n",
    "    display = 1 / (1 + np.exp(-k * (raw - c)))\n",
    "\n",
    "    return proba, heur, heur2, raw, display\n",
    "\n",
    "# -------------------------\n",
    "# QUICK TEST with your 3 samples\n",
    "# -------------------------\n",
    "bundle = joblib.load(MODEL_PATH)\n",
    "\n",
    "samples = [\n",
    "  {\n",
    "    \"student_id\": \"25\",\n",
    "    \"days_elapsed_since_reg\": 32,\n",
    "    \"clicks_per_day_total\": 2.3125,\n",
    "    \"active_ratio_total\": 0.71875,\n",
    "    \"avg_clicks_per_active_day_total\": 3.217391304347826,\n",
    "    \"days_since_last_active\": 0,\n",
    "    \"clicks_last_14_days\": 53,\n",
    "    \"active_days_14\": 14,\n",
    "    \"clicks_per_day_14\": 3.7857142857142856,\n",
    "    \"active_ratio_14\": 1.1428571428571428,  # will be clipped to 1\n",
    "    \"clicks_last_7_days\": 19,\n",
    "    \"clicks_0_7\": 10,\n",
    "    \"clicks_8_14\": 43,\n",
    "    \"trend_click_14\": 33,\n",
    "    \"ratio_click_14\": 4,\n",
    "    \"inactivity_streak_14\": 1\n",
    "  },\n",
    "  {\n",
    "    \"student_id\": \"59\",\n",
    "    \"days_elapsed_since_reg\": 26,\n",
    "    \"clicks_per_day_total\": 1.6153846153846154,\n",
    "    \"active_ratio_total\": 0.46153846153846156,\n",
    "    \"avg_clicks_per_active_day_total\": 3.5,\n",
    "    \"days_since_last_active\": 1,\n",
    "    \"clicks_last_14_days\": 42,\n",
    "    \"active_days_14\": 12,\n",
    "    \"clicks_per_day_14\": 3,\n",
    "    \"active_ratio_14\": 0.8571428571428571,\n",
    "    \"clicks_last_7_days\": 4,\n",
    "    \"clicks_0_7\": 8,\n",
    "    \"clicks_8_14\": 34,\n",
    "    \"trend_click_14\": 26,\n",
    "    \"ratio_click_14\": 3.888888888888889,\n",
    "    \"inactivity_streak_14\": 2\n",
    "  },\n",
    "  {\n",
    "    \"student_id\": \"60\",\n",
    "    \"days_elapsed_since_reg\": 31,\n",
    "    \"clicks_per_day_total\": 0.16129032258064516,\n",
    "    \"active_ratio_total\": 0.16129032258064516,\n",
    "    \"avg_clicks_per_active_day_total\": 1,\n",
    "    \"days_since_last_active\": 2,\n",
    "    \"clicks_last_14_days\": 3,\n",
    "    \"active_days_14\": 3,\n",
    "    \"clicks_per_day_14\": 0.21428571428571427,\n",
    "    \"active_ratio_14\": 0.21428571428571427,\n",
    "    \"clicks_last_7_days\": 1,\n",
    "    \"clicks_0_7\": 1,\n",
    "    \"clicks_8_14\": 2,\n",
    "    \"trend_click_14\": 1,\n",
    "    \"ratio_click_14\": 1.5,\n",
    "    \"inactivity_streak_14\": 6\n",
    "  }\n",
    "]\n",
    "bundle = joblib.load(MODEL_PATH)\n",
    "thr = bundle[\"threshold\"]\n",
    "\n",
    "inp = pd.DataFrame(samples)\n",
    "proba, heur, heur2, raw, display = predict_risk(bundle, inp)\n",
    "\n",
    "out = pd.DataFrame({\n",
    "    \"student_id\": inp[\"student_id\"].astype(str),\n",
    "    \"model_proba\": proba,\n",
    "    \"heur_boosted\": heur2,\n",
    "    \"raw_score\": raw,\n",
    "    \"display_risk\": display,\n",
    "    \"final_pred\": (raw >= bundle[\"threshold\"]).astype(int)  # dÃ¹ng raw Ä‘á»ƒ quyáº¿t Ä‘á»‹nh\n",
    "}).sort_values(\"display_risk\", ascending=False)\n",
    "\n",
    "print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ee837",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = joblib.load(MODEL_PATH)\n",
    "thr = bundle[\"threshold\"]\n",
    "\n",
    "inp = pd.DataFrame(samples)\n",
    "proba, heur, heur2, final = predict_risk(bundle, inp)\n",
    "\n",
    "out = pd.DataFrame({\n",
    "    \"student_id\": inp[\"student_id\"].astype(str),\n",
    "    \"model_proba\": proba,\n",
    "    \"heur\": heur,\n",
    "    \"heur_boosted\": heur2,\n",
    "    \"final_risk\": final,\n",
    "    \"final_pred\": (final >= thr).astype(int)\n",
    "}).sort_values(\"final_risk\", ascending=False)\n",
    "\n",
    "print(\"Saved threshold:\", thr)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08266bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-27 22:58:46,793] INFO - === DROP-OUT EARLY WARNING (DEMO LOG) ===\n",
      "[2025-12-27 22:58:46,794] INFO - Run time: 2025-12-27 22:58:46\n",
      "[2025-12-27 22:58:46,795] INFO - [NgÆ°á»i há»c A | id=25] Tá»· lá»‡ dá»± Ä‘oÃ¡n bá» há»c: 12.15% \n",
      "\t\t\t\t\t\t       -> âœ… SAN TOÃ€N (khÃ´ng gá»­i mail)\n",
      "\n",
      "[2025-12-27 22:58:46,795] INFO - [NgÆ°á»i há»c B | id=59] Tá»· lá»‡ dá»± Ä‘oÃ¡n bá» há»c: 42.34% \n",
      "\t\t\t\t\t\t       -> ðŸŸ¡ Cáº¦N NHáº®C NHá»ž (gá»­i mail nháº¯c há»c)\n",
      "\n",
      "[2025-12-27 22:58:46,796] INFO - [NgÆ°á»i há»c C | id=60] Tá»· lá»‡ dá»± Ä‘oÃ¡n bá» há»c: 81.21% \n",
      "\t\t\t\t\t\t       -> ðŸ”´ NGUY CÆ  CAO (gá»­i mail cáº£nh bÃ¡o)\n",
      "\n",
      "[2025-12-27 22:58:46,796] INFO - === END DEMO LOG ===\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(asctime)s] %(levelname)s - %(message)s\")\n",
    "\n",
    "# Fake email map (demo)\n",
    "EMAIL_MAP = {\n",
    "    25: \"learner25@example.com\",\n",
    "    59: \"learner59@example.com\",\n",
    "    60: \"learner60@example.com\",\n",
    "}\n",
    "\n",
    "def send_email_fake(to_email: str, subject: str, body: str):\n",
    "    logging.info(\"\")\n",
    "\n",
    "def log_dropout_demo():\n",
    "    # Fake prediction rates (you can change these numbers)\n",
    "    fake_predictions = [\n",
    "        {\"student_id\": 25, \"name\": \"NgÆ°á»i há»c A\", \"risk\": 0.1215, \"action\": \"SAFE\"},\n",
    "        {\"student_id\": 59, \"name\": \"NgÆ°á»i há»c B\", \"risk\": 0.4234, \"action\": \"REMIND\"},\n",
    "        {\"student_id\": 60, \"name\": \"NgÆ°á»i há»c C\", \"risk\": 0.8121, \"action\": \"ALERT\"},\n",
    "    ]\n",
    "\n",
    "    logging.info(\"=== DROP-OUT EARLY WARNING (DEMO LOG) ===\")\n",
    "    logging.info(f\"Run time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    for item in fake_predictions:\n",
    "        sid = item[\"student_id\"]\n",
    "        name = item[\"name\"]\n",
    "        risk = item[\"risk\"]\n",
    "        action = item[\"action\"]\n",
    "        email = EMAIL_MAP.get(sid, f\"learner{sid}@example.com\")\n",
    "\n",
    "        if action == \"SAFE\":\n",
    "            logging.info(f\"[{name} | id={sid}] Tá»· lá»‡ dá»± Ä‘oÃ¡n bá» há»c: {risk:.2%} \\n\\t\\t\\t\\t\\t\\t       -> âœ… SAN TOÃ€N (khÃ´ng gá»­i mail)\\n\")\n",
    "        elif action == \"REMIND\":\n",
    "            logging.info(f\"[{name} | id={sid}] Tá»· lá»‡ dá»± Ä‘oÃ¡n bá» há»c: {risk:.2%} \\n\\t\\t\\t\\t\\t\\t       -> ðŸŸ¡ Cáº¦N NHáº®C NHá»ž (gá»­i mail nháº¯c há»c)\\n\")\n",
    "           \n",
    "        elif action == \"ALERT\":\n",
    "            logging.info(f\"[{name} | id={sid}] Tá»· lá»‡ dá»± Ä‘oÃ¡n bá» há»c: {risk:.2%} \\n\\t\\t\\t\\t\\t\\t       -> ðŸ”´ NGUY CÆ  CAO (gá»­i mail cáº£nh bÃ¡o)\\n\")\n",
    "           \n",
    "\n",
    "    logging.info(\"=== END DEMO LOG ===\")\n",
    "\n",
    "# Run demo\n",
    "log_dropout_demo()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
